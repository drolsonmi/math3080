{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling\n",
    "__Math 3080: Fundamentals of Data Science__\n",
    "\n",
    "Reading:\n",
    "* [McKinney, *Python for Data Science*, Chapter 6](https://wesmckinney.com/book/accessing-data)\n",
    "* Chapters 8,10\n",
    "\n",
    "Class notes are found through GitHub. As changes are made, they will automatically be uploaded to GitHub. A link to the repository is on Canvas.\n",
    "\n",
    "-----\n",
    "## Outcomes\n",
    "### Course\n",
    "\n",
    "### Data Analysis Certification\n",
    "* Databases: Data joins\n",
    "\n",
    "-----\n",
    "## Outline\n",
    "* Concatenating\n",
    "* Mapping\n",
    "* Encoding\n",
    "    * One-hot encoding / Dummy Variables\n",
    "    * Ordinal Encoding\n",
    "* Joining\n",
    "* Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the purpose of data wrangling?\n",
    "* Combine datasets that augment our view of the data\n",
    "* Look at the data from a different perspective\n",
    "* ... other reasons ...\n",
    "\n",
    "This is one of the big skills of a Data Scientiest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "students = pd.read_excel('../Datasets/Gradebook.xlsx', sheet_name='Students')\n",
    "assignments = pd.read_excel('../Datasets/Gradebook.xlsx', sheet_name='Assignments')\n",
    "grades = pd.read_excel('../Datasets/Gradebook.xlsx', sheet_name='Grades')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(students.shape)\n",
    "students.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(assignments.shape)\n",
    "assignments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grades.shape)\n",
    "grades.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have a couple new students move in. We need to add their data to the students dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_students = pd.DataFrame({\n",
    "    'student_id' : [1101, 1102, 1103],\n",
    "    'first_name' : ['John', 'Jill', 'Jim'],\n",
    "    'last_name' : ['Anderson', 'Benson', 'Kent'],\n",
    "    'class_year' : ['Senior', 'Freshman', 'Sophomore'],\n",
    "    'gpa' : [3.41, 2.77, 3.87],\n",
    "    'major' : ['Physics', 'Biology', 'English'],\n",
    "    'first_generation_student' : ['Yes', 'Yes', 'Yes'],\n",
    "    'financial_aid' : ['Yes', 'Yes', 'Yes'],\n",
    "    'housing_status' : ['On-Campus', 'On-Campus', 'Off-Campus'],\n",
    "    'credits_completed' : [82, 7, 16]\n",
    "})\n",
    "\n",
    "new_students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([students, new_students], axis=0).tail(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(What if we don't have data on all variables? Try commenting a couple of rows from the `new_students` dictionary.)\n",
    "\n",
    "Now, let's say that in the assignments folder, we want to identify which segment of the course it is assigned in. Let's say the course is broken into 4 segments. We can do this in one of three ways: (1) concatenating, (2) directly adding columns, and (3) mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segments = pd.Series([1,1,1,2,2,2,2,3,3,3,3,4,4,4,4,1,1,2,3,3,4,2,4])\n",
    "pd.concat([assignments, segments], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments['segment'] = segments\n",
    "assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's map the assignment type from the assignments sheet onto the grades sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_dict = pd.Series(assignments['category'].values, index=assignments['assignment_id']).to_dict()\n",
    "hw_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grades['category'] = grades['assignment_id'].map(hw_dict)\n",
    "grades.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When sending data into a model, it will need numerical variables. So, we need to convert categorical variables into numerical variables that represent those categories. This process is known as __encoding__.\n",
    "\n",
    "For Nominal variables, we use __one-hot encoding__ (in Pandas, we use the `pd.get_dummies()` function to do this). It creates one column for each category and then gives it a value of 1 if the observation is in that category and a 0 if it is not.\n",
    "\n",
    "Let's do this for the students' major in the `students` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_dummies = pd.get_dummies(students['major']).astype(int)\n",
    "major_dummies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we add this onto the original dataset? We need to concatenate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = pd.concat([students, major_dummies], axis=1)\n",
    "students.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Ordinal variables, we can depict the category with a number scale (for example, pain level is often represented \"on a scale from 1 to 10\"). We can easily use this with mapping on our class level for each student."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = {\n",
    "    'Freshman' : 1,\n",
    "    'Sophomore' : 2,\n",
    "    'Junior' : 3,\n",
    "    'Senior' : 4\n",
    "}\n",
    "\n",
    "students['class_num'] = students['class_year'].map(classes)\n",
    "students.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __join__ is where you pair observations from one table with observations from another table. Let's look at these two tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "depts = pd.DataFrame({\n",
    "    'dept_id' : [10,20,30,40],\n",
    "    'dept_name' : ['Engineering','Sales','Marketing','HR']\n",
    "})\n",
    "\n",
    "display(depts)\n",
    "\n",
    "employees = pd.DataFrame({\n",
    "    'emp_id' : [1,2,3,4,5],\n",
    "    'emp_name' : ['Alice','Bob','Carol','Dan','Eve'],\n",
    "    'dept_id' : [10,20,20,50,np.nan]\n",
    "})\n",
    "\n",
    "display(employees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4 ways we can join tables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To call a join, you need to list two tables. A __left join__ will take all the entries for the first table. If there are matches in the second table, it will include that data in another column. If not, it will mark the missing data as `NaN`.\n",
    "\n",
    "<img src=\"./images/leftjoin.png\" alt=\"Left Join\" width=250, height=250>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(employees, depts, on='dept_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Right Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A __right join__ will take all the entries for the second table. If there are matches in the first table, it will include that data in another column. If not, it will mark the missing data as `NaN`.\n",
    "\n",
    "<img src=\"./images/rightjoin.png\" alt=\"Right Join\" width=250, height=250>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(employees, depts, on='dept_id', how='right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outer Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An __outer join__ will take all the entries for both tables. Any missing data is marked as `NaN`.\n",
    "\n",
    "<img src=\"./images/outerjoin.png\" alt=\"Outer Join\" width=250, height=250>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(employees, depts, on='dept_id', how='outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inner Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An __inner join__ will take only entries that are found in both tables. \n",
    "\n",
    "<img src=\"./images/innerjoin.png\" alt=\"Inner Join\" width=250, height=250>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(employees, depts, on='dept_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on column names during joins\n",
    "Sometimes, the column name used for merging is different. Let's assume that in the employees dataframe, the department was indicated differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(depts)\n",
    "\n",
    "employees = pd.DataFrame({\n",
    "    'emp_id' : [1,2,3,4,5],\n",
    "    'emp_name' : ['Alice','Bob','Carol','Dan','Eve'],\n",
    "    'department' : [10,20,20,50,np.nan]\n",
    "})\n",
    "\n",
    "display(employees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The department is still how we want to do the join. However, we have to indicate each column specifically since they are not under the same name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(employees, depts, left_on='department', right_on='dept_id', how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshaping dataframes involves reorganizing data to change how variables are arranged without altering the underlying values. This will present the data is a more suitable format for visualization and summarization.\n",
    "\n",
    "Most data is recorded in long format. With __long-format data__, variables are stacked into a single column with corresponding value and identifier columns.\n",
    "\n",
    "(With our gradbook dataset, the category is listed in the `assignment_id` column with the value in the `grade` column.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "grades = pd.read_excel('../Datasets/Gradebook.xlsx', sheet_name='Grades')\n",
    "grades.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualization, we often change long-format data into wide format. With __wide-format data__, each variable has its own column and observations are spread across many columns.\n",
    "\n",
    "(With our gradebook dataset, each category has its own column and each element represents the value for the particular identifier and category.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wide format is more useful for data analysis and preparation. However, we often prefer to store data in long format because it allows for more flexibility with how the data is used later. Let's learn how to move between long and wide formats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reshape the data using a Groupby, we are going to need a method to summarize the values in a dataset. For instance, if we have a table with the student names for rows and homework, project, and exam grades as columns, the values in the table have to summarize and represent all the homework, project, and exam entries for that student. These summary values are found using __aggregate functions__.\n",
    "\n",
    "Common aggregate functions:\n",
    "```python\n",
    "agg('max')\n",
    "agg('min')\n",
    "agg('mean') # Default\n",
    "agg('std')\n",
    "agg('count')\n",
    "```\n",
    "\n",
    "We can even create our own aggregate function\n",
    "```python    \n",
    "def range(x):\n",
    "    return x.max()-x.min()\n",
    "\n",
    "agg(range)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using one aggregate function\n",
    "print(students['gpa'].mean())\n",
    "\n",
    "print(students['gpa'].aggregate(['mean', 'min', 'median', 'max', 'std']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll see these aggregate functions in use in the following topics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with a table in long format. \n",
    "* Choose one variable to be the row in our table\n",
    "* Choose another variable to be the column in our table\n",
    "* Choose a third variable to be the value in the table\n",
    "\n",
    "If there is more than one value to go into the table, what do we do? This is where we use our aggregate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_table = pd.pivot_table(grades, index='student_id', columns='assignment_id', values='grade', aggfunc='max')\n",
    "grade_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Melting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Melting is the opposite of a pivot table. It takes the row of your table and makes that one variable, the column of your table becomes another variable, and the value becomes a third variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grade_table.melt(ignore_index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupbys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students = pd.read_excel('../Datasets/Gradebook.xlsx', sheet_name='Students')\n",
    "students.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.groupby('class_year')['gpa'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.groupby(['class_year','major'])['gpa'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "students.groupby(['major','class_year'])['gpa'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
